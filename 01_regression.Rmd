---
title: "Regression Examples"
author: "Michael Chong"
date: "2023-06-13"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# Load packages and data
library(tidyverse) 
library(MASS)
library(brms) # for fitting Bayesian regression models with R formula syntax
library(loo) # tools for validation 

data(roaches, package = "rstanarm")
```

## Poisson regression example using roach data

This example is adapted from the [`rstanarm` vignette](https://mc-stan.org/rstanarm/articles/count.html#poisson-and-negative-binomial-regression-example-1), and uses data from Gelman and Hill (2007).

We are interested in the effect of anti-pest treatment on the number of roaches in apartments.

Each of the rows in the data represent an apartment with the following columns:

* `y`, the number of roaches caught in traps post-treatment 
* `roach1`, the number of roaches caught in traps pre-treatment
* `treatment`, whether the apartment received the anti-pest treatment
* `senior`, whether the apartment building is restricted to seniors
* `exposure2`, number of days (?) where the traps are set to catch roaches

```{r}
head(roaches)

roaches$roach1 <- roaches$roach1/100
```

### Testing regular GLMs

We can fit this using a standard generalized linear model:

```{r}
poisson_glm <- glm(
  formula = y ~ roach1 + treatment + senior + offset(log(exposure2)), 
  data = roaches,
  family = "poisson"
)

summary(poisson_glm)

negbinom_glm <- MASS::glm.nb(
  formula = y ~ roach1 + treatment + senior + offset(log(exposure2)),
  data = roaches
)

negbinom_glm2 <- MASS::glm.nb(
  formula = y ~ log(roach1+.001) + treatment + senior + offset(log(exposure2)),
  data = roaches
)

summary(negbinom_glm)
```

One metric for assessment goodness-of-fit is the Akaike information criterion (AIC). Based on the way AIC is calculated in R with `AIC()`, a lower value is better.

```{r}
print("AIC for Poisson GLM ", AIC(poisson_glm), "; AIC for Neg Binom GLM ", AIC(negbinom_glm))
```

```{r}
roach_pred <- roaches |>
  mutate(
    pois = predict(poisson_glm, type = "response"),
    nb1 = predict(negbinom_glm, type = "response"),
    nb2 = predict(negbinom_glm2, type = "response")
  ) |>
  pivot_longer(cols = c("pois", "nb1", "nb2"), names_to = "model", values_to = "yhat")

ggplot(roach_pred, aes(x = y, y = yhat)) +
  geom_point(aes(colour = model), alpha = 0.3) +
  facet_wrap(~treatment)

ggplot(roach_pred |> filter(roach1 < 2), aes(x = roach1, y = yhat-y)) +
  geom_point(aes(colour = as_factor(treatment))) +
  facet_wrap(~model)
```

### Fitting in Bayes using `brms`

`brms` is an R package built to interface with `rstan` more easily. It enables specifying models using familiar R formula syntax and allows for a really wide range of models.

Below, we fit the same models as before using the `brm()` function. 

```{r}
poisson_bayes <-   brm(
  formula = y ~ roach1 + treatment + senior + offset(log(exposure2)), 
  data = roaches,
  family = "poisson"
)

negbinom_bayes <- brm(
  formula = y ~ roach1 + treatment + senior + offset(log(exposure2)),
  data = roaches,
  family = "negbinomial"
)

negbinom_bayes2 <- brm(
  formula = y ~ log(roach1+.001) + treatment + senior + offset(log(exposure2)),
  data = roaches,
  family = "negbinomial"
)

hurdle_poisson <- brm(
  formula = bf(
    y ~ roach1 + treatment + senior + offset(log(exposure2)),
    hu ~ 1 + had_roaches
  ),
  data = roaches |> mutate(had_roaches = roach1 > 0),
  family = "hurdle_poisson"
)

hurdle_negbinom <- brm(
  formula = bf(
    y ~ roach1 + treatment + senior + offset(log(exposure2)),
    hu ~ 1 + had_roaches
  ),
  data = roaches |> mutate(had_roaches = roach1 > 0),
  family = "hurdle_negbinomial"
) 

hurdle_negbinom2 <- brm(
  formula = bf(
    y ~ log(roach1 + .001) + treatment + senior + offset(log(exposure2)),
    hu ~ 1 + had_roaches
  ),
  data = roaches |> mutate(had_roaches = roach1 > 0),
  family = "hurdle_negbinomial"
) 

```

One choice of statistic that summarizes model performance is the expected log predictive density (elpd), which is a measure of out-of-sample prediction performance. We can approximate the elpd using tools from the `loo` package. 

```{r}
loo(poisson_bayes)
```

The `elpd_loo` quantity is the measure we're more interested in. It is interpreted as (**an approximation of**) the leave-one-out log predictive density, roughly meaning: if we were to one-by-one leave each data point out and try to predict it from the other data, how likely would we deem the observed value? The absolute number itself is difficult to interpret, but a higher (less negative) score is better.

The "Pareto k" values indicate which are "influential" observations for our posterior. Higher values suggest more influential data points. What this means is complicated, but can indicate a couple things:

* the approximation of the posterior with the influential observation left out is not good
* the model is heavily misspecified (and therefore unable to explain these data points)

Addressing (or ignoring) these issues depends on the context. For this tutorial we're just going to move on. 

One indication 

```{r}

```



